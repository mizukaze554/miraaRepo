<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Video → (Vocal isolate) → Whisper Transcript (Browser)</title>
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <meta http-equiv="Cross-Origin-Opener-Policy" content="same-origin" />
  <meta http-equiv="Cross-Origin-Embedder-Policy" content="require-corp" />
  <meta http-equiv="Content-Security-Policy" content="default-src 'self' blob: data: https://cdn.jsdelivr.net https://esm.sh; script-src 'self' 'unsafe-inline' 'unsafe-eval' 'wasm-unsafe-eval' https://esm.sh https://cdn.jsdelivr.net; style-src 'self' 'unsafe-inline'; worker-src 'self' blob:; connect-src 'self' https://esm.sh https://cdn.jsdelivr.net blob:; img-src 'self' blob: data:; media-src blob: 'self'; style-src-elem 'self' 'unsafe-inline'; style-src-attr 'unsafe-inline'" />
  <link rel="stylesheet" href="css/enhanced-ui.css" />
    :root {
      --primary-color: #0ea5a4;
      --error-color: #ef4444;
      --warning-color: #f59e0b;
      --success-color: #10b981;
      --bg-color: #f7fafc;
      --text-color: #0f172a;
      --border-color: #cbd5e1;
      --muted-color: #94a3b8;
      --shadow-color: rgba(0, 0, 0, 0.1);
    }

    * {
      box-sizing: border-box;
      margin: 0;
      padding: 0;
    }

    body {
      font-family: Inter, system-ui, -apple-system, Segoe UI, Roboto, "Helvetica Neue", Arial;
      max-width: 920px;
      margin: 28px auto;
      padding: 24px;
      background: var(--bg-color);
      color: var(--text-color);
      line-height: 1.5;
    }

    .container {
      background: white;
      padding: 32px;
      border-radius: 16px;
      box-shadow: 0 4px 6px var(--shadow-color);
    }

    h1 {
      font-size: 24px;
      margin: 0 0 16px;
      color: var(--text-color);
    }

    p.small {
      color: var(--muted-color);
      font-size: 14px;
      margin: 8px 0 24px;
    }

    .row {
      display: flex;
      gap: 16px;
      align-items: center;
      margin-bottom: 16px;
    }

    .col {
      display: flex;
      flex-direction: column;
      gap: 12px;
    }

    label {
      font-weight: 600;
      font-size: 14px;
      color: var(--text-color);
      margin-bottom: 4px;
    }

    select, input[type="file"] {
      padding: 10px;
      border-radius: 8px;
      border: 2px solid var(--border-color);
      background: white;
      font-size: 14px;
      transition: all 0.2s ease;
    }

    select:hover, input[type="file"]:hover {
      border-color: var(--primary-color);
    }

    button {
      padding: 12px 20px;
      border-radius: 8px;
      border: 2px solid var(--border-color);
      background: white;
      cursor: pointer;
      font-weight: 600;
      font-size: 14px;
      transition: all 0.2s ease;
      display: inline-flex;
      align-items: center;
      gap: 8px;
    }

    button:hover {
      transform: translateY(-1px);
      box-shadow: 0 2px 4px var(--shadow-color);
    }

    button.primary {
      background: var(--primary-color);
      color: white;
      border: none;
    }

    button:disabled {
      opacity: 0.6;
      cursor: not-allowed;
      transform: none;
      box-shadow: none;
    }

    pre {
      white-space: pre-wrap;
      background: var(--text-color);
      color: white;
      padding: 16px;
      border-radius: 12px;
      font-size: 14px;
      line-height: 1.6;
      max-height: 400px;
      overflow-y: auto;
    }

    .progress-container {
      margin: 24px 0;
      background: #f1f5f9;
      padding: 16px;
      border-radius: 12px;
    }

    .progress {
      height: 8px;
      background: #e2e8f0;
      border-radius: 4px;
      overflow: hidden;
      margin: 8px 0;
    }

    .bar {
      height: 100%;
      background: var(--primary-color);
      width: 0%;
      transition: width 0.3s ease;
    }

    .status {
      font-size: 14px;
      color: var(--muted-color);
      margin-top: 8px;
      transition: all 0.3s ease;
    }

    .status.error { color: var(--error-color); }
    .status.warning { color: var(--warning-color); }
    .status.success { color: var(--success-color); }

    .controls {
      display: flex;
      gap: 12px;
      flex-wrap: wrap;
      margin: 24px 0;
    }

    .note {
      font-size: 14px;
      color: var(--text-color);
      background: #f8fafc;
      padding: 16px;
      border-radius: 12px;
      border-left: 4px solid var(--primary-color);
      margin: 24px 0;
    }

    footer {
      margin-top: 32px;
      color: var(--muted-color);
      font-size: 14px;
      text-align: center;
    }

    /* Error Modal Styles */
    .error-modal {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background: rgba(0, 0, 0, 0.5);
      display: flex;
      align-items: center;
      justify-content: center;
      opacity: 0;
      transition: opacity 0.3s ease;
      z-index: 1000;
    }

    .error-modal.show {
      opacity: 1;
    }

    .error-modal.fade-out {
      opacity: 0;
    }

    .error-modal-content {
      background: white;
      border-radius: 12px;
      padding: 24px;
      max-width: 480px;
      width: 90%;
      transform: translateY(20px);
      transition: transform 0.3s ease;
    }

    .error-modal.show .error-modal-content {
      transform: translateY(0);
    }

    .error-modal-header {
      display: flex;
      justify-content: space-between;
      align-items: center;
      margin-bottom: 16px;
    }

    .error-modal-header h3 {
      margin: 0;
      color: var(--error-color);
    }

    .close-button {
      background: none;
      border: none;
      font-size: 24px;
      cursor: pointer;
      padding: 4px;
      color: var(--muted-color);
    }

    .error-modal-body {
      margin-bottom: 24px;
    }

    .error-modal-footer {
      text-align: right;
    }

    /* Video Preview Styles */
    #preview {
      width: 100%;
      max-height: 400px;
      border-radius: 12px;
      margin: 16px 0;
      background: black;
    }

    /* Checkbox Styles */
    .checkbox-wrapper {
      display: flex;
      align-items: center;
      gap: 8px;
    }

    input[type="checkbox"] {
      width: 18px;
      height: 18px;
      accent-color: var(--primary-color);
    }

    /* Loading Animation */
    @keyframes pulse {
      0% { opacity: 1; }
      50% { opacity: 0.5; }
      100% { opacity: 1; }
    }

    .loading {
      animation: pulse 1.5s infinite;
  </style>
</head>
<body>
  <h1>Video → (Optional vocal isolation) → Whisper (in-browser)</h1>
  <p class="small">Upload a video file, optionally try an approximate vocal isolation, then transcribe locally in your browser. For best results use smaller Whisper models (tiny / tiny.en).</p>

  <div class="container">
    <div class="progress-steps">
      <div class="progress-step" data-step="1">
        <span class="step-number">1</span>
        <span>Choose Video</span>
      </div>
      <div class="progress-step" data-step="2">
        <span class="step-number">2</span>
        <span>Configure</span>
      </div>
      <div class="progress-step" data-step="3">
        <span class="step-number">3</span>
        <span>Process</span>
      </div>
    </div>

    <div class="col">
      <div class="section">
        <label>Choose video file (mp4/webm/ogg)</label>
        <input id="videoFile" type="file" accept="video/*" />
        <video id="preview" controls></video>
      </div>

      <div class="section options-grid">
        <div class="option-group">
          <label for="lang">Language</label>
          <select id="lang" class="enhanced-select">
            <option value="ja">Japanese (ja)</option>
            <option value="en">English (en)</option>
            <option value="auto" selected>Auto / multilingual</option>
          </select>
        </div>

        <div class="option-group">
          <label>Model size</label>
          <select id="model" class="enhanced-select">
            <option value="Xenova/whisper-tiny.en">tiny.en (fast, english)</option>
            <option value="Xenova/whisper-tiny">tiny (multilingual, fastest)</option>
            <option value="Xenova/whisper-base">base (more accurate)</option>
            <option value="Xenova/whisper-small">small (better, slower)</option>
          </select>
        </div>

        <div class="option-group checkbox-wrapper">
          <label>
            <input id="isolate" type="checkbox" />
            Vocal isolation
          </label>
          <span class="muted">approximate (center removal)</span>
        </div>
      </div>

      <div class="controls">
        <button id="startBtn" class="primary" title="Start processing">
          Start: extract → (isolate) → transcribe
        </button>
        <button id="downloadAudio" disabled title="Download processed audio">
          Download extracted audio
        </button>
        <button id="downloadTranscript" disabled title="Download transcript">
          Download transcript
        </button>
      </div>

      <div class="progress-container">
        <div class="progress-header">
          <span class="progress-label">Progress</span>
          <span id="status" class="status">Idle</span>
        </div>
        <div class="progress" aria-hidden="true">
          <div id="prog" class="bar"></div>
        </div>
      </div>

      <div class="transcript-section">
        <label>Transcript</label>
        <pre id="transcript" class="transcript-content">—</pre>
      </div>

      <div class="note">
        <strong>Notes:</strong>
        <ul>
          <li>Models download to your browser. Pick smaller models for speed.</li>
          <li>Vocal isolation is a simple stereo center-channel subtraction — works best when vocals are mixed center.</li>
        </ul>
      </div>
    </div>
  </div>

  <footer>
    <p>Built with <strong>ffmpeg.wasm</strong> and <strong>Transformers.js / Xenova</strong></p>
    <p class="muted">No server required - all processing happens in your browser</p>
  </footer>

<script src="https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.2/dist/transformers.min.js"></script>
<script type="module" src="js/main.js"></script>

// ---------- IndexedDB Setup ----------
const DB_NAME = 'videoTranscriptDB';
const DB_VERSION = 1;
let db;

async function initDB() {
  return new Promise((resolve, reject) => {
    const request = indexedDB.open(DB_NAME, DB_VERSION);
    
    request.onerror = () => reject(request.error);
    request.onsuccess = () => {
      db = request.result;
      resolve(db);
    };
    
    request.onupgradeneeded = (event) => {
      const db = event.target.result;
      if (!db.objectStoreNames.contains('transcripts')) {
        db.createObjectStore('transcripts', { keyPath: 'id' });
      }
      if (!db.objectStoreNames.contains('audioData')) {
        db.createObjectStore('audioData', { keyPath: 'id' });
      }
    };
  });
}

// Initialize IndexedDB
initDB().catch(console.error);

async function saveToIndexedDB(storeName, data) {
  return new Promise((resolve, reject) => {
    const transaction = db.transaction([storeName], 'readwrite');
    const store = transaction.objectStore(storeName);
    const request = store.put(data);
    
    request.onsuccess = () => resolve(request.result);
    request.onerror = () => reject(request.error);
  });
}

async function getFromIndexedDB(storeName, id) {
  return new Promise((resolve, reject) => {
    const transaction = db.transaction([storeName], 'readonly');
    const store = transaction.objectStore(storeName);
    const request = store.get(id);
    
    request.onsuccess = () => resolve(request.result);
    request.onerror = () => reject(request.error);
  });
}

// ---------- Helper UI ----------
const fileEl = document.getElementById("videoFile");
const preview = document.getElementById("preview");
const startBtn = document.getElementById("startBtn");
const statusEl = document.getElementById("status");
const progBar = document.getElementById("prog");
const transcriptEl = document.getElementById("transcript");
const isolateEl = document.getElementById("isolate");
const langEl = document.getElementById("lang");
const modelEl = document.getElementById("model");
const downloadAudioBtn = document.getElementById("downloadAudio");
const downloadTranscriptBtn = document.getElementById("downloadTranscript");

let latestAudioBlob = null;
let latestTranscriptText = "";

fileEl.addEventListener("change", async (ev) => {
  const f = ev.target.files?.[0];
  if (!f) return;
  
  // Check file size (optional, set to 500MB)
  const MAX_FILE_SIZE = 500 * 1024 * 1024; // 500MB in bytes
  if (f.size > MAX_FILE_SIZE) {
    alert("File is too large. Please choose a file under 500MB.");
    fileEl.value = ''; // Reset file input
    return;
  }

  try {
    // Clean up any existing object URL
    if (preview.src) {
      URL.revokeObjectURL(preview.src);
    }

    // Create new object URL
    const url = URL.createObjectURL(f);
    
    // Set up video element
    preview.style.display = "none"; // Hide until loaded
    transcriptEl.textContent = "—";
    
    // Wait for video to be loadable
    await new Promise((resolve, reject) => {
      const timeoutId = setTimeout(() => {
        reject(new Error('Video loading timed out'));
      }, 10000); // 10 second timeout

      preview.onloadeddata = () => {
        clearTimeout(timeoutId);
        resolve();
      };
      
      preview.onerror = (e) => {
        clearTimeout(timeoutId);
        reject(new Error('Failed to load video: ' + (e.message || 'Unknown error')));
      };

      preview.src = url;
    });

    // If we get here, video loaded successfully
    preview.style.display = "block";
  } catch (error) {
    console.error('Video loading error:', error);
    alert('Error loading video. Please try a different file. ' + error.message);
    if (preview.src) {
      URL.revokeObjectURL(preview.src);
    }
    preview.src = '';
    preview.style.display = "none";
    fileEl.value = '';
  }
});

// progress helpers
function setStatus(msg, pct=null) {
  statusEl.textContent = msg;
  if (pct !== null) progBar.style.width = Math.min(100, Math.max(0, pct)) + "%";
}

// ---------- Load libraries (async) ----------
setStatus("Loading libraries (ffmpeg + transformers)...", 5);

// We'll import ffmpeg.wasm and Xenova transformers from jsdelivr CDN.
// Note: If you want to host these locally (recommended for production), download the packages and update imports.

import { createFFmpeg, fetchFile } from './js/ffmpeg.js';
const transformersImportUrl = "https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.2/dist/transformers.min.js";

let transformers;

async function loadLibraries() {
  // Load ffmpeg.wasm
  try {
    // FFmpeg is already imported above
    if (!createFFmpeg || !fetchFile) {
      throw new Error("FFmpeg failed to load");
    }
  } catch (e) {
    console.error("ffmpeg import failed:", e);
    setStatus("Error: failed to load ffmpeg.wasm. Check console for details.", 0);
    throw e;
  }

  // Load Transformers.js (Xenova)
  try {
    transformers = await import(transformersImportUrl);
  } catch (e) {
    console.error("transformers import failed:", e);
    setStatus("Error: failed to load Transformers.js. Check console for details.", 0);
    throw e;
  }

  setStatus("Libraries loaded.", 8);
}

await loadLibraries();

// ---------- ffmpeg setup ----------
const ffmpeg = createFFmpeg({
  log: true,
  progress: ({ ratio }) => {
    // ratio from 0..1
    setStatus("ffmpeg processing...", Math.round(8 + ratio * 20));
  }
});

async function ensureFFmpegLoaded() {
  if (!ffmpeg.isLoaded()) {
    setStatus("Loading ffmpeg core (wasm) — this may take a few seconds...", 10);
    // We allow ffmpeg to fetch its core files (worker + wasm). If network blocked, this will fail.
    await ffmpeg.load();
    setStatus("ffmpeg ready.", 15);
  }
}

// ---------- WAV encoder (helper) ----------
function floatTo16BitPCM(float32Array) {
  const l = float32Array.length;
  const buffer = new ArrayBuffer(l * 2);
  const view = new DataView(buffer);
  let offset = 0;
  for (let i = 0; i < l; i++, offset += 2) {
    let s = Math.max(-1, Math.min(1, float32Array[i]));
    view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7fff, true);
  }
  return new Uint8Array(buffer);
}

function encodeWAV(samples, sampleRate, numChannels=1) {
  // samples: Float32Array (interleaved if numChannels>1)
  const bytesPerSample = 2;
  const blockAlign = numChannels * bytesPerSample;
  const buffer = new ArrayBuffer(44 + samples.length * 2);
  const view = new DataView(buffer);

  /* RIFF identifier */
  writeString(view, 0, 'RIFF');
  /* file length */
  view.setUint32(4, 36 + samples.length * 2, true);
  /* RIFF type */
  writeString(view, 8, 'WAVE');
  /* format chunk identifier */
  writeString(view, 12, 'fmt ');
  /* format chunk length */
  view.setUint32(16, 16, true);
  /* sample format (raw) */
  view.setUint16(20, 1, true);
  /* channel count */
  view.setUint16(22, numChannels, true);
  /* sample rate */
  view.setUint32(24, sampleRate, true);
  /* byte rate (sampleRate * blockAlign) */
  view.setUint32(28, sampleRate * blockAlign, true);
  /* block align (channel count * bytes per sample) */
  view.setUint16(32, blockAlign, true);
  /* bits per sample */
  view.setUint16(34, 16, true);
  /* data chunk identifier */
  writeString(view, 36, 'data');
  /* data chunk length */
  view.setUint32(40, samples.length * 2, true);

  const pcm = floatTo16BitPCM(samples);
  for (let i = 0; i < pcm.length; i++) view.setUint8(44 + i, pcm[i]);

  return new Blob([view], { type: 'audio/wav' });
}

function writeString(view, offset, string) {
  for (let i = 0; i < string.length; i++) {
    view.setUint8(offset + i, string.charCodeAt(i));
  }
}

// ---------- Simple center-channel vocal isolation ----------
async function approximateVocalIsolation(wavArrayBuffer) {
  // Decode, compute (L - R)/2 to approximately remove center (or isolate center depending)
  setStatus("Decoding audio for vocal isolation...", 40);
  const audioCtx = new (window.OfflineAudioContext || window.webkitOfflineAudioContext)(2, 1, 44100);
  // Use a temporary offline context: but we need the real length, so decodeAudioData instead:
  const ctx = new (window.AudioContext || window.webkitAudioContext)();
  const decoded = await ctx.decodeAudioData(wavArrayBuffer.slice(0)); // clone
  // If mono, nothing to do
  if (decoded.numberOfChannels < 2) {
    setStatus("Audio is mono — skipping isolation.", 45);
    return wavArrayBuffer;
  }
  const left = decoded.getChannelData(0);
  const right = decoded.getChannelData(1);
  const len = Math.min(left.length, right.length);
  const out = new Float32Array(len);
  // center removal (instrumental): out = left - right
  // center isolation (vocals only): out = (left - right) * 0.5 ??? This approximates difference
  for (let i = 0; i < len; i++) {
    out[i] = (left[i] - right[i]) * 0.5;
  }
  // Resample to model's 16000 if needed when encoding? We'll keep the original sample rate and let the model accept WAV (models usually accept 16k).
  const sr = decoded.sampleRate || 44100;
  // encode to wav
  const wavBlob = encodeWAV(out, sr, 1);
  const arr = await wavBlob.arrayBuffer();
  setStatus("Vocal isolation complete.", 48);
  try { ctx.close(); } catch(e){}
  return arr;
}

// ---------- Run pipeline (Transformers.js) ----------
async function loadTranscriber(modelId) {
  setStatus(`Loading model ${modelId} (may be large) — this downloads to your browser...`, 50);
  // Use transformers.pipeline
  const { pipeline } = transformers;
  // Provide progress handler? The library may expose options to track load; we approximate via status messages
  const transcriber = await pipeline('automatic-speech-recognition', modelId, {
    // normalize: true,   // optional
    // You can add options here depending on library features.
  });
  setStatus("Model loaded.", 70);
  return transcriber;
}

async function transcribeAudioBlob(transcriber, audioBlob, langOption) {
  setStatus("Preparing audio for transcription...", 72);
  // Transformers.js typically accepts a File/Blob or audio buffer.
  // We pass a File-like object.
  const file = new File([audioBlob], "audio.wav", { type: "audio/wav" });

  setStatus("Transcribing (this may take time)...", 75);
  // If language option is provided and the model supports forced_decoder_ids, you could pass it here.
  // Many Xenova models accept an options object for the pipeline call; we'll pass basic options.
  const result = await transcriber(file, {
    // chunk_length_s: 30, stride_length_s: 5 // not all pipelines support these options
    // If user selected language, whisper models can be told to transcribe into a specific language or task.
    // For now, rely on model auto-detection or model selection (tiny.en for english).
  });
  // The pipeline returns { text: "..." } or simple string; adapt:
  const text = result?.text ?? (typeof result === 'string' ? result : JSON.stringify(result));
  setStatus("Transcription finished.", 95);
  return text;
}

// ---------- Main flow ----------
async function processVideo() {
  const f = fileEl.files?.[0];
  if (!f) { 
    alert("Please choose a video file first."); 
    return; 
  }

  startBtn.disabled = true;
  setStatus("Starting process...", 2);
  transcriptEl.textContent = "Working... (follow progress messages)";

  try {
    await ensureFFmpegLoaded();

    // write file to ffmpeg FS with chunking for large files
    const inputName = "input_vid";
    const inputExt = f.name.split('.').pop() || 'mp4';
    const inputFileName = `${inputName}.${inputExt}`;
    
    setStatus("Processing video file...", 16);
    
    const outAudioName = "extracted.wav";
    const tempFiles = [];

    try {
      // Convert file to buffer
      const fileData = await fetchFile(f);
      if (!fileData || !(fileData instanceof Uint8Array)) {
        throw new Error('Invalid file data');
      }
      
      setStatus("Writing video to ffmpeg...", 18);
      ffmpeg.FS('writeFile', inputFileName, fileData);
      tempFiles.push(inputFileName);

      setStatus("Extracting audio via ffmpeg...", 20);
      // Add -t 7200 to limit to 2 hours max, -vn no video, -ac 2 stereo, -ar 44100 sample rate, pcm_s16le
      await ffmpeg.run(
        "-y",
        "-i", inputFileName,
        "-t", "7200",
        "-vn",
        "-acodec", "pcm_s16le",
        "-ar", "44100",
        "-ac", "2",
        outAudioName
      );
      tempFiles.push(outAudioName);
    } catch (error) {
      console.error('FFmpeg processing error:', error);
      setStatus("Error processing video: " + (error?.message || String(error)), 0);
      // Clean up any temporary files
      for (const file of tempFiles) {
        try {
          ffmpeg.FS('unlink', file);
        } catch (e) {
          console.warn('Failed to clean up file:', file, e);
        }
      }
      throw error;
    }

    // Read output
    setStatus("Reading extracted audio...", 36);
    const data = ffmpeg.FS('readFile', outAudioName);
    const audioArrayBuffer = data.buffer.slice(data.byteOffset, data.byteOffset + data.byteLength);

    // optionally approximate vocal isolation
    let processedAudioBuffer = audioArrayBuffer;
    if (isolateEl.checked) {
      setStatus("Running approximate vocal isolation in-browser...", 38);
      processedAudioBuffer = await approximateVocalIsolation(audioArrayBuffer);
    }

    // create blob and keep for download
    latestAudioBlob = new Blob([processedAudioBuffer], { type: "audio/wav" });
    
    // Save to IndexedDB
    const audioId = `audio_${Date.now()}`;
    try {
      await saveToIndexedDB('audioData', {
        id: audioId,
        fileName: f.name,
        blob: latestAudioBlob,
        timestamp: Date.now()
      });

      downloadAudioBtn.disabled = false;
      downloadAudioBtn.onclick = async () => {
        try {
          const audioData = await getFromIndexedDB('audioData', audioId);
          if (audioData) {
            const url = URL.createObjectURL(audioData.blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = `${audioData.fileName.replace(/\.[^/.]+$/, "")}_audio.wav`;
            a.click();
            URL.revokeObjectURL(url);
          }
        } catch (error) {
          console.error('Error downloading audio:', error);
          alert('Error downloading audio. Please try processing the video again.');
        }
      };
    } catch (error) {
      console.error('Error saving audio to IndexedDB:', error);
      // Fall back to direct blob handling if IndexedDB fails
      downloadAudioBtn.disabled = false;
      downloadAudioBtn.onclick = () => {
        const url = URL.createObjectURL(latestAudioBlob);
        const a = document.createElement('a');
        a.href = url;
        a.download = `${f.name.replace(/\.[^/.]+$/, "")}_audio.wav`;
        a.click();
        URL.revokeObjectURL(url);
      };
    }

    // load model and transcribe
    // Choose model based on user selection. If language = en and user selected tiny.en, that's ideal.
    const selectedModel = modelEl.value;
    const transcriber = await loadTranscriber(selectedModel);

    const text = await transcribeAudioBlob(transcriber, latestAudioBlob, langEl.value);
    latestTranscriptText = text;
    transcriptEl.textContent = text || "(no text detected)";

    // Save transcript to IndexedDB
    const transcriptId = `transcript_${Date.now()}`;
    try {
      await saveToIndexedDB('transcripts', {
        id: transcriptId,
        fileName: f.name,
        text: text,
        timestamp: Date.now()
      });

      downloadTranscriptBtn.disabled = false;
      downloadTranscriptBtn.onclick = async () => {
        try {
          const transcriptData = await getFromIndexedDB('transcripts', transcriptId);
          if (transcriptData) {
            const blob = new Blob([transcriptData.text], { type: "text/plain;charset=utf-8" });
            const url = URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = `${transcriptData.fileName.replace(/\.[^/.]+$/, "")}_transcript.txt`;
            a.click();
            URL.revokeObjectURL(url);
          }
        } catch (error) {
          console.error('Error downloading transcript:', error);
          alert('Error downloading transcript. Please try processing the video again.');
        }
      };
    } catch (error) {
      console.error('Error saving transcript to IndexedDB:', error);
      // Fall back to direct text handling if IndexedDB fails
      downloadTranscriptBtn.disabled = false;
      downloadTranscriptBtn.onclick = () => {
        const blob = new Blob([latestTranscriptText], { type: "text/plain;charset=utf-8" });
        const url = URL.createObjectURL(blob);
        const a = document.createElement('a');
        a.href = url;
        a.download = `${f.name.replace(/\.[^/.]+$/, "")}_transcript.txt`;
        a.click();
        URL.revokeObjectURL(url);
      };
    }

    setStatus("Done ✅", 100);
  } catch (err) {
    console.error(err);
    setStatus("Error: " + (err?.message ?? String(err)), 0);
    transcriptEl.textContent = "Error — see console.";
    alert("Error occurred — check console for details.");
  } finally {
    startBtn.disabled = false;
  }
}

// Initialize process
startBtn.addEventListener("click", processVideo);

</script>
</body>
</html>
