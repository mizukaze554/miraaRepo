<!doctype html>
<html lang="en" class="bg-gray-50">
<head>
  <meta charset="utf-8" />
  <title>Miraa - Video to Transcript with AI</title>
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <meta http-equiv="Cross-Origin-Opener-Policy" content="same-origin" />
  <meta http-equiv="Cross-Origin-Embedder-Policy" content="require-corp" />
  <meta http-equiv="Content-Security-Policy" content="default-src 'self' blob: data: https://cdn.jsdelivr.net https://esm.sh https://unpkg.com; script-src 'self' 'unsafe-inline' 'unsafe-eval' 'wasm-unsafe-eval' https://cdn.jsdelivr.net https://esm.sh https://unpkg.com; style-src 'self' 'unsafe-inline' https://cdn.jsdelivr.net; worker-src 'self' blob:; connect-src 'self' blob: https://cdn.jsdelivr.net https://esm.sh https://unpkg.com; img-src 'self' blob: data:; media-src blob: 'self'; style-src-elem 'self' 'unsafe-inline' https://cdn.jsdelivr.net; style-src-attr 'unsafe-inline';" />
  <script src="https://cdn.jsdelivr.net/npm/@tailwindcss/browser@4"></script>
  <style>
    :root {
      --primary-color: #2563eb;
      --primary-dark: #1d4ed8;
      --error-color: #ef4444;
      --warning-color: #f59e0b;
      --success-color: #10b981;
      --bg-color: #f8fafc;
      --text-color: #0f172a;
      --border-color: #e2e8f0;
      --muted-color: #64748b;
      --shadow-color: rgba(0, 0, 0, 0.08);
    }

    body {
      font-family: system-ui, -apple-system, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, 'Noto Sans', 'Liberation Sans', sans-serif;
      -webkit-font-smoothing: antialiased;
      -moz-osx-font-smoothing: grayscale;
    }

    .glass-morphism {
      background: rgba(255, 255, 255, 0.95);
      backdrop-filter: blur(12px);
      -webkit-backdrop-filter: blur(12px);
      border: 1px solid rgba(255, 255, 255, 0.2);
    }

    .gradient-text {
      background: linear-gradient(135deg, #2563eb 0%, #4f46e5 100%);
      -webkit-background-clip: text;
      background-clip: text;
      -webkit-text-fill-color: transparent;
    }

    .drop-zone {
      border: 2px dashed var(--border-color);
      transition: all 0.3s ease;
    }

    .drop-zone:hover {
      border-color: var(--primary-color);
      background: rgba(37, 99, 235, 0.05);
    }
    

    p.small {
      color: var(--muted-color);
      font-size: 14px;
      margin: 8px 0 24px;
    }

    .row {
      display: flex;
      gap: 16px;
      align-items: center;
      margin-bottom: 16px;
    }

    .col {
      display: flex;
      flex-direction: column;
      gap: 12px;
    }

    label {
      font-weight: 600;
      font-size: 14px;
      color: var(--text-color);
      margin-bottom: 4px;
    }

    select, input[type="file"] {
      padding: 10px;
      border-radius: 8px;
      border: 2px solid var(--border-color);
      background: white;
      font-size: 14px;
      transition: all 0.2s ease;
    }

    select:hover, input[type="file"]:hover {
      border-color: var(--primary-color);
    }

    button {
      padding: 12px 20px;
      border-radius: 8px;
      border: 2px solid var(--border-color);
      background: white;
      cursor: pointer;
      font-weight: 600;
      font-size: 14px;
      transition: all 0.2s ease;
      display: inline-flex;
      align-items: center;
      gap: 8px;
    }

    button:hover {
      transform: translateY(-1px);
      box-shadow: 0 2px 4px var(--shadow-color);
    }

    button.primary {
      background: var(--primary-color);
      color: white;
      border: none;
    }

    button:disabled {
      opacity: 0.6;
      cursor: not-allowed;
      transform: none;
      box-shadow: none;
    }

    pre {
      white-space: pre-wrap;
      background: var(--text-color);
      color: white;
      padding: 16px;
      border-radius: 12px;
      font-size: 14px;
      line-height: 1.6;
      max-height: 400px;
      overflow-y: auto;
    }

    .progress-container {
      margin: 24px 0;
      background: #f1f5f9;
      padding: 16px;
      border-radius: 12px;
    }

    .progress {
      height: 8px;
      background: #e2e8f0;
      border-radius: 4px;
      overflow: hidden;
      margin: 8px 0;
    }

    .bar {
      height: 100%;
      background: var(--primary-color);
      width: 0%;
      transition: width 0.3s ease;
    }

    .status {
      font-size: 14px;
      color: var(--muted-color);
      margin-top: 8px;
      transition: all 0.3s ease;
    }

    .status.error { color: var(--error-color); }
    .status.warning { color: var(--warning-color); }
    .status.success { color: var(--success-color); }

    .controls {
      display: flex;
      gap: 12px;
      flex-wrap: wrap;
      margin: 24px 0;
    }

    .note {
      font-size: 14px;
      color: var(--text-color);
      background: #f8fafc;
      padding: 16px;
      border-radius: 12px;
      border-left: 4px solid var(--primary-color);
      margin: 24px 0;
    }

    footer {
      margin-top: 32px;
      color: var(--muted-color);
      font-size: 14px;
      text-align: center;
    }

    /* Error Modal Styles */
    .error-modal {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background: rgba(0, 0, 0, 0.5);
      display: flex;
      align-items: center;
      justify-content: center;
      opacity: 0;
      transition: opacity 0.3s ease;
      z-index: 1000;
    }

    .error-modal.show {
      opacity: 1;
    }

    .error-modal.fade-out {
      opacity: 0;
    }

    .error-modal-content {
      background: white;
      border-radius: 12px;
      padding: 24px;
      max-width: 480px;
      width: 90%;
      transform: translateY(20px);
      transition: transform 0.3s ease;
    }

    .error-modal.show .error-modal-content {
      transform: translateY(0);
    }

    .error-modal-header {
      display: flex;
      justify-content: space-between;
      align-items: center;
      margin-bottom: 16px;
    }

    .error-modal-header h3 {
      margin: 0;
      color: var(--error-color);
    }

    .close-button {
      background: none;
      border: none;
      font-size: 24px;
      cursor: pointer;
      padding: 4px;
      color: var(--muted-color);
    }

    .error-modal-body {
      margin-bottom: 24px;
    }

    .error-modal-footer {
      text-align: right;
    }

    /* Video Preview Styles */
    #preview {
      width: 100%;
      max-height: 400px;
      border-radius: 12px;
      margin: 16px 0;
      background: black;
    }

    /* Checkbox Styles */
    .checkbox-wrapper {
      display: flex;
      align-items: center;
      gap: 8px;
    }

    input[type="checkbox"] {
      width: 18px;
      height: 18px;
      accent-color: var(--primary-color);
    }

    /* Loading Animation */
    @keyframes pulse {
      0% { opacity: 1; }
      50% { opacity: 0.5; }
      100% { opacity: 1; }
    }

    .loading {
      animation: pulse 1.5s infinite;
    }
  </style>
</head>
<body>
  <div class="min-h-screen py-12 px-4 sm:px-6">
    <div class="max-w-4xl mx-auto">
      <!-- Header -->
      <div class="text-center mb-12">
        <h1 class="text-4xl font-bold mb-4 gradient-text">Miraa</h1>
        <p class="text-xl text-gray-600 mb-2">Video to Transcript with AI</p>
        <p class="text-sm text-gray-500">Extract audio, isolate vocals, and transcribe - all in your browser</p>
      </div>

      <!-- Main Container -->
      <div class="glass-morphism rounded-2xl shadow-xl overflow-hidden">
        <!-- Progress Steps -->
        <div class="border-b border-gray-100 p-6">
          <div class="max-w-3xl mx-auto">
            <div class="flex justify-between">
              <div class="flex items-center">
                <div class="flex items-center justify-center w-8 h-8 rounded-full bg-blue-600 text-white font-semibold text-sm">1</div>
                <div class="ml-3">
                  <p class="text-sm font-semibold text-gray-900">Upload</p>
                  <p class="text-xs text-gray-500">Choose video file</p>
                </div>
              </div>
              <div class="flex items-center">
                <div class="flex items-center justify-center w-8 h-8 rounded-full bg-gray-100 text-gray-600 font-semibold text-sm">2</div>
                <div class="ml-3">
                  <p class="text-sm font-semibold text-gray-600">Configure</p>
                  <p class="text-xs text-gray-500">Set preferences</p>
                </div>
              </div>
              <div class="flex items-center">
                <div class="flex items-center justify-center w-8 h-8 rounded-full bg-gray-100 text-gray-600 font-semibold text-sm">3</div>
                <div class="ml-3">
                  <p class="text-sm font-semibold text-gray-600">Process</p>
                  <p class="text-xs text-gray-500">Generate transcript</p>
                </div>
              </div>
            </div>
          </div>
        </div>

    <div class="p-6">
      <!-- File Upload Section -->
      <div class="max-w-3xl mx-auto space-y-6">
        <div class="drop-zone rounded-xl p-8 text-center cursor-pointer relative group">
          <input id="videoFile" type="file" accept="video/*,.mp4,.webm,.mov" class="absolute inset-0 w-full h-full opacity-0 cursor-pointer z-10" />
          <div class="space-y-4">
            <div id="dropIcon" class="w-16 h-16 mx-auto bg-blue-50 rounded-full flex items-center justify-center">
              <svg xmlns="http://www.w3.org/2000/svg" class="h-8 w-8 text-blue-600" viewBox="0 0 20 20" fill="currentColor">
                <path fill-rule="evenodd" d="M4 5a2 2 0 00-2 2v8a2 2 0 002 2h12a2 2 0 002-2V7a2 2 0 00-2-2h-1.586a1 1 0 01-.707-.293l-1.121-1.121A2 2 0 0011.172 3H8.828a2 2 0 00-1.414.586L6.293 4.707A1 1 0 015.586 5H4zm6 9a3 3 0 100-6 3 3 0 000 6z" clip-rule="evenodd" />
              </svg>
            </div>
            <div>
              <p class="text-sm font-medium text-gray-900">Drop your video file here</p>
              <p class="text-xs text-gray-500 mt-1">or click to browse</p>
            </div>
            <p class="text-xs text-gray-400">Supports MP4, WebM, MOV (max 500MB)</p>
          </div>
        </div>
        
        <!-- Video Preview -->
        <div class="relative">
          <video id="preview" controls class="w-full aspect-video rounded-xl bg-gray-900 hidden"></video>
        </div>

        <!-- Segments list (auto-generated after video load) -->
        <div id="segments" class="max-w-3xl mx-auto mt-4 flex gap-2 flex-wrap"></div>
        
        <!-- Subtitle overlay -->
        <div id="subtitle" class="max-w-3xl mx-auto mt-2 text-center text-white bg-black/60 p-2 rounded hidden"></div>
      </div>

      <!-- Options Grid -->
      <div class="max-w-3xl mx-auto mt-8 grid grid-cols-1 md:grid-cols-2 gap-6">
        <div class="space-y-2">
          <label for="lang" class="block text-sm font-medium text-gray-700">Language</label>
          <div class="relative">
            <select id="lang" class="block w-full pl-3 pr-10 py-2.5 text-gray-900 bg-white border border-gray-200 rounded-lg appearance-none focus:outline-none focus:ring-2 focus:ring-blue-500/20 focus:border-blue-500">
              <option value="ja">Japanese (ja)</option>
              <option value="en">English (en)</option>
              <option value="auto" selected>Auto / multilingual</option>
            </select>
            <div class="pointer-events-none absolute inset-y-0 right-0 flex items-center px-2 text-gray-400">
              <svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
              </svg>
            </div>
          </div>
        </div>

        <div class="space-y-2">
          <label for="model" class="block text-sm font-medium text-gray-700">Model Size</label>
          <div class="relative">
            <select id="model" class="block w-full pl-3 pr-10 py-2.5 text-gray-900 bg-white border border-gray-200 rounded-lg appearance-none focus:outline-none focus:ring-2 focus:ring-blue-500/20 focus:border-blue-500">
              <option value="Xenova/whisper-tiny.en">tiny.en (fast, english)</option>
              <option value="Xenova/whisper-tiny">tiny (multilingual, fastest)</option>
              <option value="Xenova/whisper-base">base (more accurate)</option>
              <option value="Xenova/whisper-small">small (better, slower)</option>
            </select>
            <div class="pointer-events-none absolute inset-y-0 right-0 flex items-center px-2 text-gray-400">
              <svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
              </svg>
            </div>
          </div>
        </div>

        <div class="md:col-span-2">
          <div class="flex items-center justify-center space-x-2 p-4 bg-gray-50 rounded-lg">
            <label class="relative inline-flex items-center cursor-pointer">
              <input id="isolate" type="checkbox" class="sr-only peer">
              <div class="w-11 h-6 bg-gray-200 peer-focus:outline-none peer-focus:ring-4 peer-focus:ring-blue-300/25 rounded-full peer peer-checked:after:translate-x-full peer-checked:after:border-white after:content-[''] after:absolute after:top-[2px] after:left-[2px] after:bg-white after:border-gray-300 after:border after:rounded-full after:h-5 after:w-5 after:transition-all peer-checked:bg-blue-600"></div>
              <span class="ml-3 text-sm font-medium text-gray-700">Enable Vocal Isolation</span>
            </label>
            <span class="text-xs text-gray-500">(Experimental)</span>
          </div>
        </div>
      </div>

      <!-- Action Buttons -->
      <div class="max-w-3xl mx-auto mt-8">
        <div class="flex flex-col sm:flex-row gap-4 justify-center">
          <button id="startBtn" class="flex-1 bg-blue-600 hover:bg-blue-700 text-white px-6 py-3 rounded-lg font-medium transition-all duration-200 transform hover:scale-[1.02] shadow-lg shadow-blue-600/20 flex items-center justify-center space-x-2">
            <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor">
              <path fill-rule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16zM9.555 7.168A1 1 0 008 8v4a1 1 0 001.555.832l3-2a1 1 0 000-1.664l-3-2z" clip-rule="evenodd" />
            </svg>
            <span>Start Processing</span>
          </button>
          <div class="flex-1 flex gap-3">
            <button id="downloadAudio" disabled class="flex-1 bg-white hover:bg-gray-50 text-gray-700 px-4 py-3 rounded-lg font-medium transition-all border border-gray-200 disabled:opacity-50 disabled:cursor-not-allowed disabled:hover:bg-white flex items-center justify-center space-x-2">
              <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 text-gray-500" viewBox="0 0 20 20" fill="currentColor">
                <path fill-rule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16zM9.555 7.168A1 1 0 008 8v4a1 1 0 001.555.832l3-2a1 1 0 000-1.664l-3-2z" clip-rule="evenodd" />
              </svg>
              <span>Audio</span>
            </button>
            <button id="downloadTranscript" disabled class="flex-1 bg-white hover:bg-gray-50 text-gray-700 px-4 py-3 rounded-lg font-medium transition-all border border-gray-200 disabled:opacity-50 disabled:cursor-not-allowed disabled:hover:bg-white flex items-center justify-center space-x-2">
              <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 text-gray-500" viewBox="0 0 20 20" fill="currentColor">
                <path fill-rule="evenodd" d="M4 4a2 2 0 012-2h4.586A2 2 0 0112 2.586L15.414 6A2 2 0 0116 7.414V16a2 2 0 01-2 2H6a2 2 0 01-2-2V4z" clip-rule="evenodd" />
              </svg>
              <span>Text</span>
            </button>
          </div>
        </div>

        <!-- Progress Section -->
        <div class="mt-8 bg-gray-50 rounded-xl p-6">
          <div class="flex justify-between items-center mb-3">
            <span class="text-sm font-medium text-gray-700">Progress</span>
            <span id="status" class="text-sm font-medium px-3 py-1 rounded-full bg-gray-100 text-gray-600">Idle</span>
          </div>
          <div class="h-2 bg-gray-200 rounded-full overflow-hidden">
            <div id="prog" class="h-full bg-blue-600 transition-all duration-300 ease-out" style="width: 0%"></div>
          </div>
        </div>
      </div>

      <!-- Transcript Section -->
      <div class="max-w-3xl mx-auto mt-8 space-y-6">
        <div class="bg-gray-900 rounded-xl overflow-hidden shadow-xl">
          <div class="flex items-center justify-between px-4 py-3 bg-gray-800">
            <div class="flex items-center space-x-2">
              <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 text-gray-400" viewBox="0 0 20 20" fill="currentColor">
                <path fill-rule="evenodd" d="M7 4a3 3 0 016 0v4a3 3 0 11-6 0V4zm4 10.93A7.001 7.001 0 0017 8a1 1 0 10-2 0A5 5 0 015 8a1 1 0 00-2 0 7.001 7.001 0 006 6.93V17H6a1 1 0 100 2h8a1 1 0 100-2h-3v-2.07z" clip-rule="evenodd" />
              </svg>
              <span class="text-sm font-medium text-gray-200">Generated Transcript</span>
            </div>
            <div class="flex items-center space-x-2"></div>
          </div>
          <pre id="transcript" class="p-4 text-gray-300 font-mono text-sm overflow-x-auto max-h-[400px] overflow-y-auto whitespace-pre-wrap">—</pre>
        </div>

        <!-- Info Cards -->
        <div class="grid grid-cols-1 md:grid-cols-2 gap-6">
          <div class="bg-blue-50 rounded-xl p-6 border border-blue-100">
            <div class="flex items-center space-x-3 mb-3">
              <div class="p-2 bg-blue-100 rounded-lg">
                <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 text-blue-700" viewBox="0 0 20 20" fill="currentColor">
                  <path d="M11 3a1 1 0 10-2 0v1a1 1 0 102 0V3zM15.657 5.757a1 1 0 00-1.414-1.414l-.707.707a1 1 0 001.414 1.414l.707-.707zM18 10a1 1 0 01-1 1h-1a1 1 0 110-2h1a1 1 0 011 1zM5.05 6.464A1 1 0 106.464 5.05l-.707-.707a1 1 0 00-1.414 1.414l.707.707zM5 10a1 1 0 01-1 1H3a1 1 0 110-2h1a1 1 0 011 1z" />
                  <path d="M10 18a1 1 0 01-1-1v-1a1 1 0 112 0v1a1 1 0 01-1 1zM4.343 14.243a1 1 0 01-1.414-1.414l.707-.707a1 1 0 111.414 1.414l-.707.707zM14.95 13.536a1 1 0 001.414 1.414l.707-.707a1 1 0 00-1.414-1.414l-.707.707z" />
                </svg>
              </div>
              <h3 class="text-sm font-semibold text-blue-900">Performance Tips</h3>
            </div>
            <ul class="space-y-2 text-sm text-blue-800">
              <li class="flex items-center space-x-2">
                <svg class="h-4 w-4 text-blue-500" viewBox="0 0 20 20" fill="currentColor">
                  <path fill-rule="evenodd" d="M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z" clip-rule="evenodd" />
                </svg>
                <span>Use smaller models for faster processing</span>
              </li>
              <li class="flex items-center space-x-2">
                <svg class="h-4 w-4 text-blue-500" viewBox="0 0 20 20" fill="currentColor">
                  <path fill-rule="evenodd" d="M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z" clip-rule="evenodd" />
                </svg>
                <span>Shorter videos process more reliably</span>
              </li>
            </ul>
          </div>

          <div class="bg-purple-50 rounded-xl p-6 border border-purple-100">
            <div class="flex items-center space-x-3 mb-3">
              <div class="p-2 bg-purple-100 rounded-lg">
                <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 text-purple-700" viewBox="0 0 20 20" fill="currentColor">
                  <path fill-rule="evenodd" d="M18 10a8 8 0 11-16 0 8 8 0 0116 0zm-8-3a1 1 0 00-.867.5 1 1 0 11-1.731-1A3 3 0 0113 8a3.001 3.001 0 01-2 2.83V11a1 1 0 11-2 0v-1a1 1 0 011-1 1 1 0 100-2zm0 8a1 1 0 100-2 1 1 0 000 2z" clip-rule="evenodd" />
                </svg>
              </div>
              <h3 class="text-sm font-semibold text-purple-900">About Vocal Isolation</h3>
            </div>
            <p class="text-sm text-purple-800">
              Vocal isolation uses center-channel extraction and works best when vocals are mixed in the center of the stereo field.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>

  <footer class="py-8 text-center space-y-4">
    <div class="flex items-center justify-center space-x-3">
      <span class="px-3 py-1 text-xs font-medium text-blue-700 bg-blue-100 rounded-full">ffmpeg.wasm</span>
      <span class="px-3 py-1 text-xs font-medium text-purple-700 bg-purple-100 rounded-full">Transformers.js</span>
      <span class="px-3 py-1 text-xs font-medium text-green-700 bg-green-100 rounded-full">100% Browser-based</span>
    </div>
    <p class="text-sm text-gray-500">All processing happens in your browser - no server required</p>
  </footer>

  <!-- Update script import at the end of body -->
  <script type="module" src="js/main.js"></script>

  <!-- Auto-subtitle prebuffer + Web Speech API transcription (10s segments) -->
  <script type="module">
  // filepath: /home/mizui/Workspace/hobby/miraa/index.html (inline module)

  const preview = document.getElementById('preview');
  const transcriptEl = document.getElementById('transcript');
  const subtitleEl = document.getElementById('subtitle');

  // segment size in seconds
  const SEG_LEN = 10;

  // store raw bytes (Uint8Array) and blob for each segment index
  const segmentBytes = {}; // idx -> Uint8Array
  const segmentBlobs = {}; // idx -> Blob
  const segmentTranscripts = {}; // idx -> string

  // Record a segment by creating a hidden video, seeking to start, captureStream and MediaRecorder
  async function recordSegmentBlob(srcUrl, startSec, durationSec) {
    // if already recorded, return blob
    const idxKey = `${startSec.toFixed(3)}`;
    if (segmentBlobs[idxKey]) return segmentBlobs[idxKey];

    const v = document.createElement('video');
    v.src = srcUrl;
    v.muted = true;
    v.playsInline = true;
    v.preload = 'auto';
    v.style.display = 'none';
    document.body.appendChild(v);

    // wait metadata
    await new Promise((res, rej) => {
      const t = setTimeout(() => rej(new Error('Video load timeout')), 10000);
      v.addEventListener('loadedmetadata', () => { clearTimeout(t); res(); }, { once: true });
      v.addEventListener('error', () => { clearTimeout(t); rej(new Error('Video load error')); }, { once: true });
    });

    // seek to start
    await new Promise((res) => {
      const onSeek = () => { v.removeEventListener('seeked', onSeek); res(); };
      v.addEventListener('seeked', onSeek);
      try { v.currentTime = Math.min(startSec, Math.max(0, v.duration - 0.001)); } catch (e) { res(); }
      setTimeout(res, 1500);
    });

    try { await v.play(); } catch (_) { /* some browsers require user interaction; still try */ }

    let stream = null;
    try { stream = v.captureStream(); } catch (err) {
      stream = v.mozCaptureStream ? v.mozCaptureStream() : null;
    }
    if (!stream) {
      v.pause(); v.remove();
      throw new Error('captureStream not supported');
    }

    const audioTracks = stream.getAudioTracks();
    if (!audioTracks || audioTracks.length === 0) {
      v.pause(); v.remove();
      throw new Error('No audio track available');
    }
    const audioStream = new MediaStream(audioTracks);

    let mimeType = '';
    if (MediaRecorder.isTypeSupported && MediaRecorder.isTypeSupported('audio/webm;codecs=opus')) mimeType = 'audio/webm;codecs=opus';
    else if (MediaRecorder.isTypeSupported && MediaRecorder.isTypeSupported('audio/webm')) mimeType = 'audio/webm';
    else if (MediaRecorder.isTypeSupported && MediaRecorder.isTypeSupported('audio/ogg')) mimeType = 'audio/ogg';

    const recorder = new MediaRecorder(audioStream, mimeType ? { mimeType } : undefined);
    const chunks = [];
    recorder.ondataavailable = (e) => { if (e.data && e.data.size) chunks.push(e.data); };

    recorder.start();

    // stop after durationSec or when video ends
    await new Promise((res) => {
      const stopAfter = setTimeout(() => {
        try { recorder.stop(); } catch (_) {}
        res();
      }, Math.max(100, durationSec * 1000));

      v.addEventListener('ended', () => {
        clearTimeout(stopAfter);
        try { recorder.stop(); } catch (_) {}
        res();
      }, { once: true });
    });

    // wait for finalization
    await new Promise((res) => {
      recorder.onstop = () => res();
    });

    const blob = new Blob(chunks, { type: mimeType || 'audio/webm' });
    // store blob and raw bytes
    const arrayBuffer = await blob.arrayBuffer();
    const u8 = new Uint8Array(arrayBuffer);
    segmentBytes[idxKey] = u8;
    segmentBlobs[idxKey] = blob;

    // cleanup
    v.pause();
    v.remove();

    return blob;
  }

  // Transcribe a blob by playing it and using Web Speech Recognition to listen while it plays.
  // Note: SpeechRecognition listens from microphone; this method plays the audio so recognition can pick it up.
  async function transcribeBlobViaSpeechRecognition(blob, lang = 'en-US', timeoutMs = 20000) {
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    if (!SpeechRecognition) throw new Error('SpeechRecognition not supported');
    return new Promise((resolve, reject) => {
      const url = URL.createObjectURL(blob);
      const audio = new Audio(url);
      audio.crossOrigin = 'anonymous';
      audio.volume = 1.0;

      let finalText = '';
      let interimText = '';

      const recognition = new SpeechRecognition();
      try { recognition.lang = lang; } catch (_) {}
      recognition.continuous = true;
      recognition.interimResults = true;
      recognition.maxAlternatives = 1;

      let finished = false;
      const cleanup = () => {
        try { recognition.stop(); } catch (_) {}
        try { audio.pause(); } catch (_) {}
        URL.revokeObjectURL(url);
      };

      recognition.onresult = (event) => {
        interimText = '';
        for (let i = event.resultIndex; i < event.results.length; ++i) {
          const r = event.results[i];
          if (r.isFinal) {
            finalText += r[0].transcript + ' ';
          } else {
            interimText += r[0].transcript + ' ';
          }
        }
        // show interim in transcript area while processing
        transcriptEl.textContent = (finalText + interimText).trim();
      };

      recognition.onerror = (ev) => {
        console.warn('SpeechRecognition error', ev);
      };

      recognition.onend = () => {
        if (finished) return;
        finished = true;
        cleanup();
        resolve((finalText || '').trim());
      };

      // start recognition then play
      try {
        recognition.start();
      } catch (err) {
        cleanup();
        reject(err);
        return;
      }

      audio.play().catch((err) => {
        console.warn('audio.play failed', err);
        // still wait for recognition events
      });

      // safety timeout
      setTimeout(() => {
        if (finished) return;
        finished = true;
        cleanup();
        resolve((finalText || '').trim());
      }, timeoutMs);
    });
  }

  // Helper: render subtitle words uniformly for a given transcript and segment duration
  function renderSubtitleForSegmentText(text, segStart, segDuration) {
    if (!text) {
      subtitleEl.classList.add('hidden');
      subtitleEl.innerHTML = '';
      return;
    }
    const words = text.trim().split(/\s+/).filter(Boolean);
    if (words.length === 0) {
      subtitleEl.classList.add('hidden');
      subtitleEl.innerHTML = '';
      return;
    }
    subtitleEl.classList.remove('hidden');
    subtitleEl.innerHTML = words.map((w,i)=>`<span data-word="${i}" style="opacity:0.6">${escapeHtml(w)}</span>`).join(' ');
    // attach timeupdate handler to highlight across segDuration when video is playing over that segStart
    function onTime() {
      const t = preview.currentTime;
      const elapsed = Math.max(0, t - segStart);
      const ratio = Math.min(1, segDuration ? (elapsed/segDuration) : 0);
      const upto = Math.floor((words.length || 1) * ratio);
      const spans = subtitleEl.querySelectorAll('span[data-word]');
      spans.forEach(s => {
        const idx = Number(s.getAttribute('data-word'));
        s.style.opacity = idx < upto ? '1' : '0.6';
        s.style.fontWeight = idx < upto ? '700' : '400';
      });
      // If playback left this segment, remove handler
      if (t < segStart - 0.05 || t > segStart + segDuration + 0.05) {
        preview.removeEventListener('timeupdate', onTime);
      }
    }
    // remove any previous and add fresh
    preview.addEventListener('timeupdate', onTime);
    // run once to initialize
    onTime();
  }

  // escape HTML
  function escapeHtml(s) { return String(s).replace(/[&<>"']/g, c => ({'&':'&amp;','<':'&lt;','>':'&gt;','"':'&quot;',"'":"&#39;"}[c])); }

  // compute segment index from time
  function segIndexFromTime(t) { return Math.floor(t / SEG_LEN); }

  // when video metadata loaded, create segments (no heavy recording yet)
  preview.addEventListener('loadedmetadata', () => {
    // nothing heavy here; we will record lazily
    const total = preview.duration || 0;
    const count = Math.ceil(total / SEG_LEN);
    // show initial placeholder subtitle for first segment (pre-create one segment transcript early)
    if (count > 0) {
      prefetchAndTranscribeSegment(0); // start transcribing first segment early
    }
  });

  // When playback enters a new segment, show available subtitles and pre-transcribe the next segment
  let lastShownSegment = -1;
  preview.addEventListener('timeupdate', async () => {
    const t = preview.currentTime;
    const idx = segIndexFromTime(t);
    if (idx !== lastShownSegment) {
      lastShownSegment = idx;
      const segStart = idx * SEG_LEN;
      const segDur = Math.min(SEG_LEN, (preview.duration || 0) - segStart);
      const key = `${segStart.toFixed(3)}`;

      // If transcript ready, render it
      if (segmentTranscripts[key]) {
        renderSubtitleForSegmentText(segmentTranscripts[key], segStart, segDur);
      } else {
        // show "Processing..." placeholder while it's being produced
        subtitleEl.classList.remove('hidden');
        subtitleEl.innerHTML = `<span style="opacity:0.8">[Processing segment ${idx+1}...]</span>`;
        // ensure prefetch for this segment is started
        prefetchAndTranscribeSegment(idx);
      }

      // prefetch next segment (10s early)
      prefetchAndTranscribeSegment(idx + 1);
    }
  });

  // Prefetch and transcribe specified segment index (if within duration)
  async function prefetchAndTranscribeSegment(idx) {
    const total = preview.duration || 0;
    const segStart = idx * SEG_LEN;
    if (segStart >= total || idx < 0) return;
    const segDur = Math.min(SEG_LEN, total - segStart);
    const key = `${segStart.toFixed(3)}`;
    if (segmentTranscripts[key]) return; // already have transcript
    try {
      // ensure we have blob/bytes
      let blob = segmentBlobs[key];
      if (!blob) {
        const srcUrl = preview.currentSrc || preview.src;
        blob = await recordSegmentBlob(srcUrl, segStart, segDur);
      }
      // store bytes if not set
      if (!segmentBytes[key]) {
        const ab = await blob.arrayBuffer();
        segmentBytes[key] = new Uint8Array(ab);
      }
      // transcribe using Web Speech API (requires mic permission)
      // choose language from UI
      const lang = (document.getElementById('lang') && document.getElementById('lang').value) || 'en-US';
      const result = await transcribeBlobViaSpeechRecognition(blob, (lang === 'auto' ? 'en-US' : lang), Math.ceil(segDur * 1000) + 8000);
      segmentTranscripts[key] = result || '[no speech detected]';
      // If current video time is within this segment, immediately render
      const curT = preview.currentTime;
      if (curT >= segStart && curT <= segStart + segDur + 0.05) {
        renderSubtitleForSegmentText(segmentTranscripts[key], segStart, segDur);
      }
      // also append to main transcript area (append with segment time)
      const stamp = new Date().toISOString();
      transcriptEl.textContent = transcriptEl.textContent + `\n[${segStart.toFixed(1)}s] ${segmentTranscripts[key]}`;
    } catch (err) {
      console.warn('prefetch/transcribe failed for segment', idx, err);
      // mark as failed to avoid retry storm
      segmentTranscripts[key] = '[transcription failed]';
      transcriptEl.textContent = transcriptEl.textContent + `\n[${segStart.toFixed(1)}s] [transcription failed]`;
    }
  }

  // NOTE: Web Speech API listens to microphone; to transcribe audio played by the page
  // the browser will capture the played audio via the system mic. There is no standard
  // SpeechRecognition API method to feed raw audio bytes directly.
  // This implementation records audio bytes (Uint8Array) and plays them back while
  // SpeechRecognition listens so it can produce transcripts. The user must allow microphone permission.

  </script>

</body>
</html>

