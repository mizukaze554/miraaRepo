<!doctype html>
<html lang="en" class="bg-gray-50">
<head>
  <meta charset="utf-8" />
  <title>Miraa - Video to Transcript with AI</title>
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <meta http-equiv="Cross-Origin-Opener-Policy" content="same-origin" />
  <meta http-equiv="Cross-Origin-Embedder-Policy" content="require-corp" />
  <meta http-equiv="Content-Security-Policy" content="default-src 'self' blob: data: https://cdn.jsdelivr.net https://esm.sh; script-src 'self' 'unsafe-inline' 'unsafe-eval' 'wasm-unsafe-eval' https://esm.sh https://cdn.jsdelivr.net; style-src 'self' 'unsafe-inline' https://cdn.jsdelivr.net; worker-src 'self' blob:; connect-src 'self' https://esm.sh https://cdn.jsdelivr.net blob:; img-src 'self' blob: data:; media-src blob: 'self'; style-src-elem 'self' 'unsafe-inline' https://cdn.jsdelivr.net; style-src-attr 'unsafe-inline'" />
  <script src="https://cdn.jsdelivr.net/npm/@tailwindcss/browser@4"></script>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
  <style>
    :root {
      --primary-color: #2563eb;
      --primary-dark: #1d4ed8;
      --error-color: #ef4444;
      --warning-color: #f59e0b;
      --success-color: #10b981;
      --bg-color: #f8fafc;
      --text-color: #0f172a;
      --border-color: #e2e8f0;
      --muted-color: #64748b;
      --shadow-color: rgba(0, 0, 0, 0.08);
    }

    body {
      font-family: 'Inter', system-ui, -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
      -webkit-font-smoothing: antialiased;
      -moz-osx-font-smoothing: grayscale;
    }

    .glass-morphism {
      background: rgba(255, 255, 255, 0.95);
      backdrop-filter: blur(12px);
      -webkit-backdrop-filter: blur(12px);
      border: 1px solid rgba(255, 255, 255, 0.2);
    }

    .gradient-text {
      background: linear-gradient(135deg, #2563eb 0%, #4f46e5 100%);
      -webkit-background-clip: text;
      background-clip: text;
      -webkit-text-fill-color: transparent;
    }

    .drop-zone {
      border: 2px dashed var(--border-color);
      transition: all 0.3s ease;
    }

    .drop-zone:hover {
      border-color: var(--primary-color);
      background: rgba(37, 99, 235, 0.05);
    }
    

    p.small {
      color: var(--muted-color);
      font-size: 14px;
      margin: 8px 0 24px;
    }

    .row {
      display: flex;
      gap: 16px;
      align-items: center;
      margin-bottom: 16px;
    }

    .col {
      display: flex;
      flex-direction: column;
      gap: 12px;
    }

    label {
      font-weight: 600;
      font-size: 14px;
      color: var(--text-color);
      margin-bottom: 4px;
    }

    select, input[type="file"] {
      padding: 10px;
      border-radius: 8px;
      border: 2px solid var(--border-color);
      background: white;
      font-size: 14px;
      transition: all 0.2s ease;
    }

    select:hover, input[type="file"]:hover {
      border-color: var(--primary-color);
    }

    button {
      padding: 12px 20px;
      border-radius: 8px;
      border: 2px solid var(--border-color);
      background: white;
      cursor: pointer;
      font-weight: 600;
      font-size: 14px;
      transition: all 0.2s ease;
      display: inline-flex;
      align-items: center;
      gap: 8px;
    }

    button:hover {
      transform: translateY(-1px);
      box-shadow: 0 2px 4px var(--shadow-color);
    }

    button.primary {
      background: var(--primary-color);
      color: white;
      border: none;
    }

    button:disabled {
      opacity: 0.6;
      cursor: not-allowed;
      transform: none;
      box-shadow: none;
    }

    pre {
      white-space: pre-wrap;
      background: var(--text-color);
      color: white;
      padding: 16px;
      border-radius: 12px;
      font-size: 14px;
      line-height: 1.6;
      max-height: 400px;
      overflow-y: auto;
    }

    .progress-container {
      margin: 24px 0;
      background: #f1f5f9;
      padding: 16px;
      border-radius: 12px;
    }

    .progress {
      height: 8px;
      background: #e2e8f0;
      border-radius: 4px;
      overflow: hidden;
      margin: 8px 0;
    }

    .bar {
      height: 100%;
      background: var(--primary-color);
      width: 0%;
      transition: width 0.3s ease;
    }

    .status {
      font-size: 14px;
      color: var(--muted-color);
      margin-top: 8px;
      transition: all 0.3s ease;
    }

    .status.error { color: var(--error-color); }
    .status.warning { color: var(--warning-color); }
    .status.success { color: var(--success-color); }

    .controls {
      display: flex;
      gap: 12px;
      flex-wrap: wrap;
      margin: 24px 0;
    }

    .note {
      font-size: 14px;
      color: var(--text-color);
      background: #f8fafc;
      padding: 16px;
      border-radius: 12px;
      border-left: 4px solid var(--primary-color);
      margin: 24px 0;
    }

    footer {
      margin-top: 32px;
      color: var(--muted-color);
      font-size: 14px;
      text-align: center;
    }

    /* Error Modal Styles */
    .error-modal {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background: rgba(0, 0, 0, 0.5);
      display: flex;
      align-items: center;
      justify-content: center;
      opacity: 0;
      transition: opacity 0.3s ease;
      z-index: 1000;
    }

    .error-modal.show {
      opacity: 1;
    }

    .error-modal.fade-out {
      opacity: 0;
    }

    .error-modal-content {
      background: white;
      border-radius: 12px;
      padding: 24px;
      max-width: 480px;
      width: 90%;
      transform: translateY(20px);
      transition: transform 0.3s ease;
    }

    .error-modal.show .error-modal-content {
      transform: translateY(0);
    }

    .error-modal-header {
      display: flex;
      justify-content: space-between;
      align-items: center;
      margin-bottom: 16px;
    }

    .error-modal-header h3 {
      margin: 0;
      color: var(--error-color);
    }

    .close-button {
      background: none;
      border: none;
      font-size: 24px;
      cursor: pointer;
      padding: 4px;
      color: var(--muted-color);
    }

    .error-modal-body {
      margin-bottom: 24px;
    }

    .error-modal-footer {
      text-align: right;
    }

    /* Video Preview Styles */
    #preview {
      width: 100%;
      max-height: 400px;
      border-radius: 12px;
      margin: 16px 0;
      background: black;
    }

    /* Checkbox Styles */
    .checkbox-wrapper {
      display: flex;
      align-items: center;
      gap: 8px;
    }

    input[type="checkbox"] {
      width: 18px;
      height: 18px;
      accent-color: var(--primary-color);
    }

    /* Loading Animation */
    @keyframes pulse {
      0% { opacity: 1; }
      50% { opacity: 0.5; }
      100% { opacity: 1; }
    }

    .loading {
      animation: pulse 1.5s infinite;
    }
  </style>
</head>
<body>
  <div class="min-h-screen py-12 px-4 sm:px-6">
    <div class="max-w-4xl mx-auto">
      <!-- Header -->
      <div class="text-center mb-12">
        <h1 class="text-4xl font-bold mb-4 gradient-text">Miraa</h1>
        <p class="text-xl text-gray-600 mb-2">Video to Transcript with AI</p>
        <p class="text-sm text-gray-500">Extract audio, isolate vocals, and transcribe - all in your browser</p>
      </div>

      <!-- Main Container -->
      <div class="glass-morphism rounded-2xl shadow-xl overflow-hidden">
        <!-- Progress Steps -->
        <div class="border-b border-gray-100 p-6">
          <div class="max-w-3xl mx-auto">
            <div class="flex justify-between">
              <div class="flex items-center">
                <div class="flex items-center justify-center w-8 h-8 rounded-full bg-blue-600 text-white font-semibold text-sm">1</div>
                <div class="ml-3">
                  <p class="text-sm font-semibold text-gray-900">Upload</p>
                  <p class="text-xs text-gray-500">Choose video file</p>
                </div>
              </div>
              <div class="flex items-center">
                <div class="flex items-center justify-center w-8 h-8 rounded-full bg-gray-100 text-gray-600 font-semibold text-sm">2</div>
                <div class="ml-3">
                  <p class="text-sm font-semibold text-gray-600">Configure</p>
                  <p class="text-xs text-gray-500">Set preferences</p>
                </div>
              </div>
              <div class="flex items-center">
                <div class="flex items-center justify-center w-8 h-8 rounded-full bg-gray-100 text-gray-600 font-semibold text-sm">3</div>
                <div class="ml-3">
                  <p class="text-sm font-semibold text-gray-600">Process</p>
                  <p class="text-xs text-gray-500">Generate transcript</p>
                </div>
              </div>
            </div>
          </div>
        </div>

    <div class="p-6">
      <!-- File Upload Section -->
      <div class="max-w-3xl mx-auto space-y-6">
        <div class="drop-zone rounded-xl p-8 text-center cursor-pointer relative group">
          <input id="videoFile" type="file" accept="video/*" class="absolute inset-0 w-full h-full opacity-0 cursor-pointer z-10" />
          <div class="space-y-4">
            <div class="w-16 h-16 mx-auto bg-blue-50 rounded-full flex items-center justify-center">
              <svg xmlns="http://www.w3.org/2000/svg" class="h-8 w-8 text-blue-600" viewBox="0 0 20 20" fill="currentColor">
                <path fill-rule="evenodd" d="M4 5a2 2 0 00-2 2v8a2 2 0 002 2h12a2 2 0 002-2V7a2 2 0 00-2-2h-1.586a1 1 0 01-.707-.293l-1.121-1.121A2 2 0 0011.172 3H8.828a2 2 0 00-1.414.586L6.293 4.707A1 1 0 015.586 5H4zm6 9a3 3 0 100-6 3 3 0 000 6z" clip-rule="evenodd" />
              </svg>
            </div>
            <div>
              <p class="text-sm font-medium text-gray-900">Drop your video file here</p>
              <p class="text-xs text-gray-500 mt-1">or click to browse</p>
            </div>
            <p class="text-xs text-gray-400">Supports MP4, WebM, MOV (max 500MB)</p>
          </div>
        </div>
        
        <!-- Video Preview -->
        <div class="relative">
          <video id="preview" controls class="w-full aspect-video rounded-xl bg-gray-900 hidden"></video>
        </div>
      </div>

      <!-- Options Grid -->
      <div class="max-w-3xl mx-auto mt-8 grid grid-cols-1 md:grid-cols-2 gap-6">
        <div class="space-y-2">
          <label for="lang" class="block text-sm font-medium text-gray-700">Language</label>
          <div class="relative">
            <select id="lang" class="block w-full pl-3 pr-10 py-2.5 text-gray-900 bg-white border border-gray-200 rounded-lg appearance-none focus:outline-none focus:ring-2 focus:ring-blue-500/20 focus:border-blue-500">
              <option value="ja">Japanese (ja)</option>
              <option value="en">English (en)</option>
              <option value="auto" selected>Auto / multilingual</option>
            </select>
            <div class="pointer-events-none absolute inset-y-0 right-0 flex items-center px-2 text-gray-400">
              <svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
              </svg>
            </div>
          </div>
        </div>

        <div class="space-y-2">
          <label for="model" class="block text-sm font-medium text-gray-700">Model Size</label>
          <div class="relative">
            <select id="model" class="block w-full pl-3 pr-10 py-2.5 text-gray-900 bg-white border border-gray-200 rounded-lg appearance-none focus:outline-none focus:ring-2 focus:ring-blue-500/20 focus:border-blue-500">
              <option value="Xenova/whisper-tiny.en">tiny.en (fast, english)</option>
              <option value="Xenova/whisper-tiny">tiny (multilingual, fastest)</option>
              <option value="Xenova/whisper-base">base (more accurate)</option>
              <option value="Xenova/whisper-small">small (better, slower)</option>
            </select>
            <div class="pointer-events-none absolute inset-y-0 right-0 flex items-center px-2 text-gray-400">
              <svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
              </svg>
            </div>
          </div>
        </div>

        <div class="md:col-span-2">
          <div class="flex items-center justify-center space-x-2 p-4 bg-gray-50 rounded-lg">
            <label class="relative inline-flex items-center cursor-pointer">
              <input id="isolate" type="checkbox" class="sr-only peer">
              <div class="w-11 h-6 bg-gray-200 peer-focus:outline-none peer-focus:ring-4 peer-focus:ring-blue-300/25 rounded-full peer peer-checked:after:translate-x-full peer-checked:after:border-white after:content-[''] after:absolute after:top-[2px] after:left-[2px] after:bg-white after:border-gray-300 after:border after:rounded-full after:h-5 after:w-5 after:transition-all peer-checked:bg-blue-600"></div>
              <span class="ml-3 text-sm font-medium text-gray-700">Enable Vocal Isolation</span>
            </label>
            <span class="text-xs text-gray-500">(Experimental)</span>
          </div>
        </div>
      </div>

      <!-- Action Buttons -->
      <div class="max-w-3xl mx-auto mt-8">
        <div class="flex flex-col sm:flex-row gap-4 justify-center">
          <button id="startBtn" class="flex-1 bg-blue-600 hover:bg-blue-700 text-white px-6 py-3 rounded-lg font-medium transition-all duration-200 transform hover:scale-[1.02] shadow-lg shadow-blue-600/20 flex items-center justify-center space-x-2">
            <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor">
              <path fill-rule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16zM9.555 7.168A1 1 0 008 8v4a1 1 0 001.555.832l3-2a1 1 0 000-1.664l-3-2z" clip-rule="evenodd" />
            </svg>
            <span>Start Processing</span>
          </button>
          <div class="flex-1 flex gap-3">
            <button id="downloadAudio" disabled class="flex-1 bg-white hover:bg-gray-50 text-gray-700 px-4 py-3 rounded-lg font-medium transition-all border border-gray-200 disabled:opacity-50 disabled:cursor-not-allowed disabled:hover:bg-white flex items-center justify-center space-x-2">
              <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 text-gray-500" viewBox="0 0 20 20" fill="currentColor">
                <path fill-rule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16zM9.555 7.168A1 1 0 008 8v4a1 1 0 001.555.832l3-2a1 1 0 000-1.664l-3-2z" clip-rule="evenodd" />
              </svg>
              <span>Audio</span>
            </button>
            <button id="downloadTranscript" disabled class="flex-1 bg-white hover:bg-gray-50 text-gray-700 px-4 py-3 rounded-lg font-medium transition-all border border-gray-200 disabled:opacity-50 disabled:cursor-not-allowed disabled:hover:bg-white flex items-center justify-center space-x-2">
              <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 text-gray-500" viewBox="0 0 20 20" fill="currentColor">
                <path fill-rule="evenodd" d="M4 4a2 2 0 012-2h4.586A2 2 0 0112 2.586L15.414 6A2 2 0 0116 7.414V16a2 2 0 01-2 2H6a2 2 0 01-2-2V4z" clip-rule="evenodd" />
              </svg>
              <span>Text</span>
            </button>
          </div>
        </div>

        <!-- Progress Section -->
        <div class="mt-8 bg-gray-50 rounded-xl p-6">
          <div class="flex justify-between items-center mb-3">
            <span class="text-sm font-medium text-gray-700">Progress</span>
            <span id="status" class="text-sm font-medium px-3 py-1 rounded-full bg-gray-100 text-gray-600">Idle</span>
          </div>
          <div class="h-2 bg-gray-200 rounded-full overflow-hidden">
            <div id="prog" class="h-full bg-blue-600 transition-all duration-300 ease-out" style="width: 0%"></div>
          </div>
        </div>
      </div>

      <!-- Transcript Section -->
      <div class="max-w-3xl mx-auto mt-8 space-y-6">
        <div class="bg-gray-900 rounded-xl overflow-hidden shadow-xl">
          <div class="flex items-center justify-between px-4 py-3 bg-gray-800">
            <div class="flex items-center space-x-2">
              <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 text-gray-400" viewBox="0 0 20 20" fill="currentColor">
                <path fill-rule="evenodd" d="M7 4a3 3 0 016 0v4a3 3 0 11-6 0V4zm4 10.93A7.001 7.001 0 0017 8a1 1 0 10-2 0A5 5 0 015 8a1 1 0 00-2 0 7.001 7.001 0 006 6.93V17H6a1 1 0 100 2h8a1 1 0 100-2h-3v-2.07z" clip-rule="evenodd" />
              </svg>
              <span class="text-sm font-medium text-gray-200">Generated Transcript</span>
            </div>
            <div class="flex items-center space-x-2"></div>
          </div>
          <pre id="transcript" class="p-4 text-gray-300 font-mono text-sm overflow-x-auto max-h-[400px] overflow-y-auto whitespace-pre-wrap">—</pre>
        </div>

        <!-- Info Cards -->
        <div class="grid grid-cols-1 md:grid-cols-2 gap-6">
          <div class="bg-blue-50 rounded-xl p-6 border border-blue-100">
            <div class="flex items-center space-x-3 mb-3">
              <div class="p-2 bg-blue-100 rounded-lg">
                <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 text-blue-700" viewBox="0 0 20 20" fill="currentColor">
                  <path d="M11 3a1 1 0 10-2 0v1a1 1 0 102 0V3zM15.657 5.757a1 1 0 00-1.414-1.414l-.707.707a1 1 0 001.414 1.414l.707-.707zM18 10a1 1 0 01-1 1h-1a1 1 0 110-2h1a1 1 0 011 1zM5.05 6.464A1 1 0 106.464 5.05l-.707-.707a1 1 0 00-1.414 1.414l.707.707zM5 10a1 1 0 01-1 1H3a1 1 0 110-2h1a1 1 0 011 1z" />
                  <path d="M10 18a1 1 0 01-1-1v-1a1 1 0 112 0v1a1 1 0 01-1 1zM4.343 14.243a1 1 0 01-1.414-1.414l.707-.707a1 1 0 111.414 1.414l-.707.707zM14.95 13.536a1 1 0 001.414 1.414l.707-.707a1 1 0 00-1.414-1.414l-.707.707z" />
                </svg>
              </div>
              <h3 class="text-sm font-semibold text-blue-900">Performance Tips</h3>
            </div>
            <ul class="space-y-2 text-sm text-blue-800">
              <li class="flex items-center space-x-2">
                <svg class="h-4 w-4 text-blue-500" viewBox="0 0 20 20" fill="currentColor">
                  <path fill-rule="evenodd" d="M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z" clip-rule="evenodd" />
                </svg>
                <span>Use smaller models for faster processing</span>
              </li>
              <li class="flex items-center space-x-2">
                <svg class="h-4 w-4 text-blue-500" viewBox="0 0 20 20" fill="currentColor">
                  <path fill-rule="evenodd" d="M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z" clip-rule="evenodd" />
                </svg>
                <span>Shorter videos process more reliably</span>
              </li>
            </ul>
          </div>

          <div class="bg-purple-50 rounded-xl p-6 border border-purple-100">
            <div class="flex items-center space-x-3 mb-3">
              <div class="p-2 bg-purple-100 rounded-lg">
                <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 text-purple-700" viewBox="0 0 20 20" fill="currentColor">
                  <path fill-rule="evenodd" d="M18 10a8 8 0 11-16 0 8 8 0 0116 0zm-8-3a1 1 0 00-.867.5 1 1 0 11-1.731-1A3 3 0 0113 8a3.001 3.001 0 01-2 2.83V11a1 1 0 11-2 0v-1a1 1 0 011-1 1 1 0 100-2zm0 8a1 1 0 100-2 1 1 0 000 2z" clip-rule="evenodd" />
                </svg>
              </div>
              <h3 class="text-sm font-semibold text-purple-900">About Vocal Isolation</h3>
            </div>
            <p class="text-sm text-purple-800">
              Vocal isolation uses center-channel extraction and works best when vocals are mixed in the center of the stereo field.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>

  <footer class="py-8 text-center space-y-4">
    <div class="flex items-center justify-center space-x-3">
      <span class="px-3 py-1 text-xs font-medium text-blue-700 bg-blue-100 rounded-full">ffmpeg.wasm</span>
      <span class="px-3 py-1 text-xs font-medium text-purple-700 bg-purple-100 rounded-full">Transformers.js</span>
      <span class="px-3 py-1 text-xs font-medium text-green-700 bg-green-100 rounded-full">100% Browser-based</span>
    </div>
    <p class="text-sm text-gray-500">All processing happens in your browser - no server required</p>
  </footer>

<script type="module" src="js/main.js"></script>
<script type="module">

async function initDB() {
  return new Promise((resolve, reject) => {
    const request = indexedDB.open(DB_NAME, DB_VERSION);
    
    request.onerror = () => reject(request.error);
    request.onsuccess = () => {
      db = request.result;
      resolve(db);
    };
    
    request.onupgradeneeded = (event) => {
      const db = event.target.result;
      if (!db.objectStoreNames.contains('transcripts')) {
        db.createObjectStore('transcripts', { keyPath: 'id' });
      }
      if (!db.objectStoreNames.contains('audioData')) {
        db.createObjectStore('audioData', { keyPath: 'id' });
      }
    };
  });
}

// Initialize IndexedDB
initDB().catch(console.error);

async function saveToIndexedDB(storeName, data) {
  return new Promise((resolve, reject) => {
    const transaction = db.transaction([storeName], 'readwrite');
    const store = transaction.objectStore(storeName);
    const request = store.put(data);
    
    request.onsuccess = () => resolve(request.result);
    request.onerror = () => reject(request.error);
  });
}

async function getFromIndexedDB(storeName, id) {
  return new Promise((resolve, reject) => {
    const transaction = db.transaction([storeName], 'readonly');
    const store = transaction.objectStore(storeName);
    const request = store.get(id);
    
    request.onsuccess = () => resolve(request.result);
    request.onerror = () => reject(request.error);
  });
}

// ---------- Helper UI ----------
const fileEl = document.getElementById("videoFile");
const preview = document.getElementById("preview");
const startBtn = document.getElementById("startBtn");
const statusEl = document.getElementById("status");
const progBar = document.getElementById("prog");
const transcriptEl = document.getElementById("transcript");
const isolateEl = document.getElementById("isolate");
const langEl = document.getElementById("lang");
const modelEl = document.getElementById("model");
const downloadAudioBtn = document.getElementById("downloadAudio");
const downloadTranscriptBtn = document.getElementById("downloadTranscript");

let latestAudioBlob = null;
let latestTranscriptText = "";

fileEl.addEventListener("change", async (ev) => {
  const f = ev.target.files?.[0];
  if (!f) return;
  
  // Check file size (optional, set to 500MB)
  const MAX_FILE_SIZE = 500 * 1024 * 1024; // 500MB in bytes
  if (f.size > MAX_FILE_SIZE) {
    alert("File is too large. Please choose a file under 500MB.");
    fileEl.value = ''; // Reset file input
    return;
  }

  try {
    // Clean up any existing object URL
    if (preview.src) {
      URL.revokeObjectURL(preview.src);
    }

    // Create new object URL
    const url = URL.createObjectURL(f);
    
    // Set up video element
    preview.style.display = "none"; // Hide until loaded
    transcriptEl.textContent = "—";
    
    // Wait for video to be loadable
    await new Promise((resolve, reject) => {
      const timeoutId = setTimeout(() => {
        reject(new Error('Video loading timed out'));
      }, 10000); // 10 second timeout

      preview.onloadeddata = () => {
        clearTimeout(timeoutId);
        resolve();
      };
      
      preview.onerror = (e) => {
        clearTimeout(timeoutId);
        reject(new Error('Failed to load video: ' + (e.message || 'Unknown error')));
      };

      preview.src = url;
    });

    // If we get here, video loaded successfully
    preview.style.display = "block";
  } catch (error) {
    console.error('Video loading error:', error);
    alert('Error loading video. Please try a different file. ' + error.message);
    if (preview.src) {
      URL.revokeObjectURL(preview.src);
    }
    preview.src = '';
    preview.style.display = "none";
    fileEl.value = '';
  }
});

// progress helpers
function setStatus(msg, pct=null) {
  statusEl.textContent = msg;
  if (pct !== null) progBar.style.width = Math.min(100, Math.max(0, pct)) + "%";
}

// ---------- Load libraries (async) ----------
setStatus("Loading libraries (ffmpeg + transformers)...", 5);

// We'll import ffmpeg.wasm and Xenova transformers from jsdelivr CDN.
// Note: If you want to host these locally (recommended for production), download the packages and update imports.

import { createFFmpeg, fetchFile } from './js/ffmpeg.js';
const transformersImportUrl = "https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.2/dist/transformers.min.js";

let transformers;

async function loadLibraries() {
  // Load ffmpeg.wasm
  try {
    // FFmpeg is already imported above
    if (!createFFmpeg || !fetchFile) {
      throw new Error("FFmpeg failed to load");
    }
  } catch (e) {
    console.error("ffmpeg import failed:", e);
    setStatus("Error: failed to load ffmpeg.wasm. Check console for details.", 0);
    throw e;
  }

  // Load Transformers.js (Xenova)
  try {
    transformers = await import(transformersImportUrl);
  } catch (e) {
    console.error("transformers import failed:", e);
    setStatus("Error: failed to load Transformers.js. Check console for details.", 0);
    throw e;
  }

  setStatus("Libraries loaded.", 8);
}

await loadLibraries();

// ---------- ffmpeg setup ----------
const ffmpeg = createFFmpeg({
  log: true,
  progress: ({ ratio }) => {
    // ratio from 0..1
    setStatus("ffmpeg processing...", Math.round(8 + ratio * 20));
  }
});

async function ensureFFmpegLoaded() {
  if (!ffmpeg.isLoaded()) {
    setStatus("Loading ffmpeg core (wasm) — this may take a few seconds...", 10);
    // We allow ffmpeg to fetch its core files (worker + wasm). If network blocked, this will fail.
    await ffmpeg.load();
    setStatus("ffmpeg ready.", 15);
  }
}

// ---------- WAV encoder (helper) ----------
function floatTo16BitPCM(float32Array) {
  const l = float32Array.length;
  const buffer = new ArrayBuffer(l * 2);
  const view = new DataView(buffer);
  let offset = 0;
  for (let i = 0; i < l; i++, offset += 2) {
    let s = Math.max(-1, Math.min(1, float32Array[i]));
    view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7fff, true);
  }
  return new Uint8Array(buffer);
}

function encodeWAV(samples, sampleRate, numChannels=1) {
  // samples: Float32Array (interleaved if numChannels>1)
  const bytesPerSample = 2;
  const blockAlign = numChannels * bytesPerSample;
  const buffer = new ArrayBuffer(44 + samples.length * 2);
  const view = new DataView(buffer);

  /* RIFF identifier */
  writeString(view, 0, 'RIFF');
  /* file length */
  view.setUint32(4, 36 + samples.length * 2, true);
  /* RIFF type */
  writeString(view, 8, 'WAVE');
  /* format chunk identifier */
  writeString(view, 12, 'fmt ');
  /* format chunk length */
  view.setUint32(16, 16, true);
  /* sample format (raw) */
  view.setUint16(20, 1, true);
  /* channel count */
  view.setUint16(22, numChannels, true);
  /* sample rate */
  view.setUint32(24, sampleRate, true);
  /* byte rate (sampleRate * blockAlign) */
  view.setUint32(28, sampleRate * blockAlign, true);
  /* block align (channel count * bytes per sample) */
  view.setUint16(32, blockAlign, true);
  /* bits per sample */
  view.setUint16(34, 16, true);
  /* data chunk identifier */
  writeString(view, 36, 'data');
  /* data chunk length */
  view.setUint32(40, samples.length * 2, true);

  const pcm = floatTo16BitPCM(samples);
  for (let i = 0; i < pcm.length; i++) view.setUint8(44 + i, pcm[i]);

  return new Blob([view], { type: 'audio/wav' });
}

function writeString(view, offset, string) {
  for (let i = 0; i < string.length; i++) {
    view.setUint8(offset + i, string.charCodeAt(i));
  }
}

// ---------- Simple center-channel vocal isolation ----------
async function approximateVocalIsolation(wavArrayBuffer) {
  // Decode, compute (L - R)/2 to approximately remove center (or isolate center depending)
  setStatus("Decoding audio for vocal isolation...", 40);
  const audioCtx = new (window.OfflineAudioContext || window.webkitOfflineAudioContext)(2, 1, 44100);
  // Use a temporary offline context: but we need the real length, so decodeAudioData instead:
  const ctx = new (window.AudioContext || window.webkitAudioContext)();
  const decoded = await ctx.decodeAudioData(wavArrayBuffer.slice(0)); // clone
  // If mono, nothing to do
  if (decoded.numberOfChannels < 2) {
    setStatus("Audio is mono — skipping isolation.", 45);
    return wavArrayBuffer;
  }
  const left = decoded.getChannelData(0);
  const right = decoded.getChannelData(1);
  const len = Math.min(left.length, right.length);
  const out = new Float32Array(len);
  // center removal (instrumental): out = left - right
  // center isolation (vocals only): out = (left - right) * 0.5 ??? This approximates difference
  for (let i = 0; i < len; i++) {
    out[i] = (left[i] - right[i]) * 0.5;
  }
  // Resample to model's 16000 if needed when encoding? We'll keep the original sample rate and let the model accept WAV (models usually accept 16k).
  const sr = decoded.sampleRate || 44100;
  // encode to wav
  const wavBlob = encodeWAV(out, sr, 1);
  const arr = await wavBlob.arrayBuffer();
  setStatus("Vocal isolation complete.", 48);
  try { ctx.close(); } catch(e){}
  return arr;
}

// ---------- Run pipeline (Transformers.js) ----------
async function loadTranscriber(modelId) {
  setStatus(`Loading model ${modelId} (may be large) — this downloads to your browser...`, 50);
  // Use transformers.pipeline
  const { pipeline } = transformers;
  // Provide progress handler? The library may expose options to track load; we approximate via status messages
  const transcriber = await pipeline('automatic-speech-recognition', modelId, {
    // normalize: true,   // optional
    // You can add options here depending on library features.
  });
  setStatus("Model loaded.", 70);
  return transcriber;
}

async function transcribeAudioBlob(transcriber, audioBlob, langOption) {
  setStatus("Preparing audio for transcription...", 72);
  // Transformers.js typically accepts a File/Blob or audio buffer.
  // We pass a File-like object.
  const file = new File([audioBlob], "audio.wav", { type: "audio/wav" });

  setStatus("Transcribing (this may take time)...", 75);
  // If language option is provided and the model supports forced_decoder_ids, you could pass it here.
  // Many Xenova models accept an options object for the pipeline call; we'll pass basic options.
  const result = await transcriber(file, {
    // chunk_length_s: 30, stride_length_s: 5 // not all pipelines support these options
    // If user selected language, whisper models can be told to transcribe into a specific language or task.
    // For now, rely on model auto-detection or model selection (tiny.en for english).
  });
  // The pipeline returns { text: "..." } or simple string; adapt:
  const text = result?.text ?? (typeof result === 'string' ? result : JSON.stringify(result));
  setStatus("Transcription finished.", 95);
  return text;
}

// ---------- Main flow ----------
async function processVideo() {
  const f = fileEl.files?.[0];
  if (!f) { 
    alert("Please choose a video file first."); 
    return; 
  }

  startBtn.disabled = true;
  setStatus("Starting process...", 2);
  transcriptEl.textContent = "Working... (follow progress messages)";

  try {
    await ensureFFmpegLoaded();

    // write file to ffmpeg FS with chunking for large files
    const inputName = "input_vid";
    const inputExt = f.name.split('.').pop() || 'mp4';
    const inputFileName = `${inputName}.${inputExt}`;
    
    setStatus("Processing video file...", 16);
    
    const outAudioName = "extracted.wav";
    const tempFiles = [];

    try {
      // Convert file to buffer
      const fileData = await fetchFile(f);
      if (!fileData || !(fileData instanceof Uint8Array)) {
        throw new Error('Invalid file data');
      }
      
      setStatus("Writing video to ffmpeg...", 18);
      ffmpeg.FS('writeFile', inputFileName, fileData);
      tempFiles.push(inputFileName);

      setStatus("Extracting audio via ffmpeg...", 20);
      // Add -t 7200 to limit to 2 hours max, -vn no video, -ac 2 stereo, -ar 44100 sample rate, pcm_s16le
      await ffmpeg.run(
        "-y",
        "-i", inputFileName,
        "-t", "7200",
        "-vn",
        "-acodec", "pcm_s16le",
        "-ar", "44100",
        "-ac", "2",
        outAudioName
      );
      tempFiles.push(outAudioName);
    } catch (error) {
      console.error('FFmpeg processing error:', error);
      setStatus("Error processing video: " + (error?.message || String(error)), 0);
      // Clean up any temporary files
      for (const file of tempFiles) {
        try {
          ffmpeg.FS('unlink', file);
        } catch (e) {
          console.warn('Failed to clean up file:', file, e);
        }
      }
      throw error;
    }

    // Read output
    setStatus("Reading extracted audio...", 36);
    const data = ffmpeg.FS('readFile', outAudioName);
    const audioArrayBuffer = data.buffer.slice(data.byteOffset, data.byteOffset + data.byteLength);

    // optionally approximate vocal isolation
    let processedAudioBuffer = audioArrayBuffer;
    if (isolateEl.checked) {
      setStatus("Running approximate vocal isolation in-browser...", 38);
      processedAudioBuffer = await approximateVocalIsolation(audioArrayBuffer);
    }

    // create blob and keep for download
    latestAudioBlob = new Blob([processedAudioBuffer], { type: "audio/wav" });
    
    // Save to IndexedDB
    const audioId = `audio_${Date.now()}`;
    try {
      await saveToIndexedDB('audioData', {
        id: audioId,
        fileName: f.name,
        blob: latestAudioBlob,
        timestamp: Date.now()
      });

      downloadAudioBtn.disabled = false;
      downloadAudioBtn.onclick = async () => {
        try {
          const audioData = await getFromIndexedDB('audioData', audioId);
          if (audioData) {
            const url = URL.createObjectURL(audioData.blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = `${audioData.fileName.replace(/\.[^/.]+$/, "")}_audio.wav`;
            a.click();
            URL.revokeObjectURL(url);
          }
        } catch (error) {
          console.error('Error downloading audio:', error);
          alert('Error downloading audio. Please try processing the video again.');
        }
      };
    } catch (error) {
      console.error('Error saving audio to IndexedDB:', error);
      // Fall back to direct blob handling if IndexedDB fails
      downloadAudioBtn.disabled = false;
      downloadAudioBtn.onclick = () => {
        const url = URL.createObjectURL(latestAudioBlob);
        const a = document.createElement('a');
        a.href = url;
        a.download = `${f.name.replace(/\.[^/.]+$/, "")}_audio.wav`;
        a.click();
        URL.revokeObjectURL(url);
      };
    }

    // load model and transcribe
    // Choose model based on user selection. If language = en and user selected tiny.en, that's ideal.
    const selectedModel = modelEl.value;
    const transcriber = await loadTranscriber(selectedModel);

    const text = await transcribeAudioBlob(transcriber, latestAudioBlob, langEl.value);
    latestTranscriptText = text;
    transcriptEl.textContent = text || "(no text detected)";

    // Save transcript to IndexedDB
    const transcriptId = `transcript_${Date.now()}`;
    try {
      await saveToIndexedDB('transcripts', {
        id: transcriptId,
        fileName: f.name,
        text: text,
        timestamp: Date.now()
      });

      downloadTranscriptBtn.disabled = false;
      downloadTranscriptBtn.onclick = async () => {
        try {
          const transcriptData = await getFromIndexedDB('transcripts', transcriptId);
          if (transcriptData) {
            const blob = new Blob([transcriptData.text], { type: "text/plain;charset=utf-8" });
            const url = URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = `${transcriptData.fileName.replace(/\.[^/.]+$/, "")}_transcript.txt`;
            a.click();
            URL.revokeObjectURL(url);
          }
        } catch (error) {
          console.error('Error downloading transcript:', error);
          alert('Error downloading transcript. Please try processing the video again.');
        }
      };
    } catch (error) {
      console.error('Error saving transcript to IndexedDB:', error);
      // Fall back to direct text handling if IndexedDB fails
      downloadTranscriptBtn.disabled = false;
      downloadTranscriptBtn.onclick = () => {
        const blob = new Blob([latestTranscriptText], { type: "text/plain;charset=utf-8" });
        const url = URL.createObjectURL(blob);
        const a = document.createElement('a');
        a.href = url;
        a.download = `${f.name.replace(/\.[^/.]+$/, "")}_transcript.txt`;
        a.click();
        URL.revokeObjectURL(url);
      };
    }

    setStatus("Done ✅", 100);
  } catch (err) {
    console.error(err);
    setStatus("Error: " + (err?.message ?? String(err)), 0);
    transcriptEl.textContent = "Error — see console.";
    alert("Error occurred — check console for details.");
  } finally {
    startBtn.disabled = false;
  }
}

// Initialize process
startBtn.addEventListener("click", processVideo);

</script>
</body>
</html>
